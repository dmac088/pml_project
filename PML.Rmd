---
title: "Practical Machine Learning - Coursera Assignment"
output: html_document
date: "2023-02-12"
---

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = FALSE)
```

## Introduction

The purpose of the project is to predict the manner in "which they did the exercise", which means to predict the dependent variable "classe". The overall approach will be as follows:

-   Analyze the data and make a judgement call on the preparedness of the dataset.

-   Preprocess the data for model training by casting datatypes appropriately, impute missing values.

-   Separate the data-set into testing and training subsets.

-   Perform feature reduction using lasso (for main effects).

-   Train the model on the training data-set using cross validation.

-   Test the model on the testing data-set and check the out of sample error.

-   Run the model over the 20 test cases and see what accuracy we get.

```{r}
options(warn=-1)
set.seed(123456);
```

------------------------------------------------------------------------

## Analyze the data

In total there are 19622 observations in the data-set, with 159 potential predictor variables. Given that there are so many potential predictor variables we should apply feature reduction techniques such as Lasso. But before we get to that lets pre-process it.

```{r}
dim(data);
```

------------------------------------------------------------------------

## Preprocess the data

The following predictors/features were removed from the data:

-   **"X"**: which is clearly just a sequence number, and not present in out cases data-set.

-   **"raw_timestamp_part_1"**: this defines *"when"* the exercise was performed, not "how".

-   **"raw_timestamp_part_2"**: this defines *"when"* the exercise was performed, not "how".

-   **"cvtd_timestamp"**: this defines *"when"* the exercise was performed, not "how".

-   **"user_name"**: this defines *"who"* did the exercise, not "how" is was done.

-   **"num_window"**: this does not seem to define *"how"* the exercise was performed.

-   **"new_window"**: this does not seem to define *"how"* the exercise was performed.

For the remaining perdictors that define *"how"* the exercise was performed, we need to cast them to numeric type where applicable.

***Note:*** total_accel_forearm for some reason causes errors when training a lasso model, therefore it is excluded from the data.

```{r, warning = FALSE}
library(caret);

#download the data required for this assignment
data = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv");

#remove candidate predictors
data <- data[, !(colnames(data) %in% c("X", "raw_timestamp_part_1", 
"raw_timestamp_part_2", "cvtd_timestamp", "user_name", "num_window", "new_window","total_accel_forearm"))];

#cast candidate predictors
data <- data.frame(sapply(data[,!(colnames(data) %in% c("classe"))],as.numeric), classe = data$classe);

#remove any predictors that are over 95% missing (NA)
data <- data[lapply(data, function(x) sum(is.na(x)) / length(x)) < 0.05];

data$classe <- as.factor(data$classe)

#drop columns where all rows have missing data
data <- data[,colSums(is.na(data))<nrow(data)];

#impute missing values with a median
preObj <- preProcess(data[,which(colnames(data)=="classe")*-1], method="medianImpute");
data <- predict(preObj, data);

#check out dimensionality
dim(data);
```

We now have just 51 candidate predictors, so the cleanup was definitely worthwhile.

------------------------------------------------------------------------

## Split the data

Split the data into testing and training subsets in the usual way.

```{r, warning = FALSE}
inTrain <- createDataPartition(data$classe,
                               p=3/4,
                               list = FALSE);

training <- data[inTrain,];
testing <- data[-inTrain,];
```

------------------------------------------------------------------------

## Feature reduction and selection

Train a Lasso model using all the predictors available in the data-set. Lasso is a nice way to automate feature selection. We use k-fold cross-validation with k=5 to reduce training set bias (prevent overfitting). With k=5 20% of the training data is held out for testing in each iteration.

```{r, warning = FALSE}
library(glmnet);
control <- trainControl(method='repeatedcv',
                        number=5,
                        repeats=3);
modFit <- train(classe ~ ., data=training, method="glmnet", family="multinomial", type.multinomial = "grouped", trControl=control, standardize = TRUE);
modFit;
```

Check the predictor importance using plotmo, which will show us the predictors retained in the model for each increase in lambda. Each predictor will have different effects on the outcome "classe", therefore we take a look at the plot for each outcome independently, however notice that there is some commonality between them.

```{r, warning = FALSE}
library(plotmo);
plot_glmnet(modFit$finalModel, nresponse=1)
plot_glmnet(modFit$finalModel, nresponse=2)
plot_glmnet(modFit$finalModel, nresponse=3)
plot_glmnet(modFit$finalModel, nresponse=4)
plot_glmnet(modFit$finalModel, nresponse=5)
```

Next, we check the predictor importance across all outcomes using varImp, and ensure we use the optimal lambda from training, that being the lambda value that yields a model having the highest predictive accuracy.

```{r, warning = FALSE}
library(caret)
lassoImp <- varImp(modFit, lambda=0.0003572849);
lassoImp
```

We can see from below plots that the importance drops off significantly after 10-12 predictors, therefore keeping the 20 most important to carry over into a random forest should be sufficient. It's possible the random forest will have higher predictive power, especially if there are non-linear relationships with "classe".

```{r, warning = FALSE}
plot(lassoImp, top = 20)
```

------------------------------------------------------------------------

## Train a random forest model

Next, we train a random forest model using cross-validation and carry over the 20 important predictors identified by the Lasso in previous step. A random forest is chosen for this classification problem, since it can elegantly handle co-linearity, sparsity, and non-normality in the predictor variables.

```{r, warning = FALSE}
#keep only the top 20 predictor variables in the model, this will help prevent over-fitting
p = plot(lassoImp, top=20);
lst = c(p$y.limits, "classe");
training.optimized <- training[, (colnames(training) %in% lst)];

#Train the model
rfFit <- train(classe ~ ., data=training.optimized, method="rf", trControl=control);
rfFit
```

Caret found an optimal tree model at mtry=2 with accuracy of 0.98. However we should be cautious about the high accuracy and test the model on unseen data to evaluate the true performance.

------------------------------------------------------------------------

## Test the model

We see below that the model performs extremely well on the training data with perfect accuracy, however this could mean that the model is over-fitted to the training data-set, but since we've used cross-validation our final model should be OK to proceed to test on the testing/evaluation data-set. I don't expect such perfect results on the unseen data.

```{r, warning = FALSE}
#test for accuracy
training.optimized <- training[, (colnames(training) %in% lst)];
rfPred <- predict(rfFit, training.optimized);
confusionMatrix(as.factor(training.optimized$classe), rfPred);
```

Next we test the model on unseen data (testing/evaluation data-set) to evaluate the model performance. The accuracy should be very close our training set since we used k=5 cross-validation to reduce over-fitting. I expect the "out of sample" error to be higher than the "in sample" error, so let's check out the confusion matrix.

```{r, warning = FALSE}
#test for accuracy
testing.optimized <- testing[, (colnames(testing) %in% lst)];
rfPred <- predict(rfFit, newdata=testing.optimized);
confusionMatrix(as.factor(testing.optimized$classe), rfPred);
```

The accuracy on the testing/evaluation data-set is 0.9849, which is highly accurate, although not quite as accurate as what's achieved in the literature This model should be good enough for us to pass the final test on the 20 cases.

```{r}
#predict the result of the 20 test cases
cases = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv");

cases.optimized <- cases[, (colnames(cases) %in% c(lst, "problem_id"))];
sapply(cases.optimized[,!(colnames(cases.optimized) %in% c("problem_id"))], as.numeric);

#impute any missing values
cases.optimized <- predict(preObj, cases.optimized);

results <- data.frame(problem_id = cases.optimized$problem_id, 
                      prediction = predict(rfFit, cases.optimized)
);
results;
```

The model resulted in an almost perfect score of 19/20 for the cases.

------------------------------------------------------------------------
