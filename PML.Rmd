---
title: "Practical Machine Learning - Coursera Assignment"
output: html_document
date: "2023-02-12"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The purpose of the project is to predict the manner in which they did the exercise, which means to predict the dependent variable "classe". The overall approach will be as follows:

-   Analyze the data and make a judgement call on the preparedness of the dataset.

-   Preprocess the data for model training by casting datatypes appropriately, impute missing values

-   Separate the dataset into testing and training subsets.

-   Train a model on the training dataset using cross validation.

-   Test the model on the testing dataset and check the out of sample error.

-   Run the model over the 20 test cases and see what accuracy we get.

## Analyzing the data

In total we have 19622 observations in the dataset, with 164 potential predictor variables/columns. Given that there are so many potential predictor variables a random forest is a good coice for modelling on this dataset, since a RF can assist in feature selection by determining which predictors are significant and add predictive power to the model. However to proceed we need to preProcess the data for training.

## Preprocessing the data

After analyzing that data it's apparent that there are timestamp variables that cannot be consumed by our caret train function, therefore we need to split cvtd_timestamp into year, month, day, hour, minute, and cast each new predictor columns to numeric.

```{r cars}
options(warn=-1)

#download the data required for the assignment
data = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv");
cases = read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv");

#install and load caret
#install.packages("caret");
library(caret);

data$year <- as.numeric(substr(data$cvtd_timestamp, 7, 11)); 
data$month <- as.numeric(substr(data$cvtd_timestamp, 4, 5)); 
data$day <- as.numeric(substr(data$cvtd_timestamp, 1, 2)); 
data$hour <- as.numeric(substr(data$cvtd_timestamp, 12, 13)); 
data$minute <- as.numeric(substr(data$cvtd_timestamp, 15, 16));

#remove predictor variables that are redundant in our dataset
data = data[, !(colnames(data) %in% c("X", "raw_timestamp_part_1", 
"raw_timestamp_part_2", "cvtd_timestamp"))];

#cast predictors with incorrect datatypes. I'm unsure why this happens
data$kurtosis_roll_belt       <- as.numeric(data$kurtosis_roll_belt);
data$kurtosis_picth_belt      <- as.numeric(data$kurtosis_picth_belt);
data$kurtosis_yaw_belt        <- as.numeric(data$kurtosis_yaw_belt);
data$skewness_roll_belt       <- as.numeric(data$skewness_roll_belt);
data$skewness_roll_belt.1     <- as.numeric(data$skewness_roll_belt.1);
data$skewness_yaw_belt        <- as.numeric(data$skewness_yaw_belt);
data$max_yaw_belt             <- as.numeric(data$max_yaw_belt);
data$min_yaw_belt1            <- as.numeric(data$min_yaw_belt);
data$amplitude_yaw_belt       <- as.numeric(data$amplitude_yaw_belt);
data$kurtosis_yaw_arm         <- as.numeric(data$kurtosis_yaw_arm);
data$skewness_roll_arm        <- as.numeric(data$skewness_roll_arm);
data$skewness_pitch_arm       <- as.numeric(data$skewness_pitch_arm);
data$skewness_yaw_arm         <- as.numeric(data$skewness_yaw_arm);
data$kurtosis_roll_dumbbell   <- as.numeric(data$kurtosis_roll_dumbbell);
data$kurtosis_picth_dumbbell  <- as.numeric(data$kurtosis_picth_dumbbel);
data$kurtosis_yaw_dumbbell    <- as.numeric(data$kurtosis_yaw_dumbbell);
data$skewness_roll_dumbbell   <- as.numeric(data$skewness_roll_dumbbell);
data$skewness_pitch_dumbbell  <- as.numeric(data$skewness_pitch_dumbbell);
data$skewness_yaw_dumbbell    <- as.numeric(data$skewness_yaw_dumbbell);
data$max_yaw_dumbbell         <- as.numeric(data$max_yaw_dumbbell);
data$min_yaw_dumbbell         <- as.numeric(data$min_yaw_dumbbell);
data$amplitude_yaw_dumbbell   <- as.numeric(data$amplitude_yaw_dumbbell);
data$kurtosis_roll_forearm    <- as.numeric(data$kurtosis_roll_forearm);
data$kurtosis_picth_forearm   <- as.numeric(data$kurtosis_picth_forearm);
data$kurtosis_yaw_torearm     <- as.numeric(data$kurtosis_yaw_forearm);
data$skewness_roll_forearm    <- as.numeric(data$skewness_roll_forearm);
data$skewness_pitch_forearm   <- as.numeric(data$skewness_pitch_forearm);
data$skewnessyawforearm       <- as.numeric(data$skewness_yaw_forearm);
data$max_yaw_forearm          <- as.numeric(data$max_yaw_forearm);
data$min_yaw_forearm          <- as.numeric(data$min_yaw_forearm);
data$amplitude_yaw_forearm    <- as.numeric(data$amplitude_yaw_forearm);
data$kurtosis_roll_arm        <- as.numeric(data$kurtosis_roll_arm);
data$kurtosis_picth_arm       <- as.numeric(data$kurtosis_picth_arm);
```

## Splitting the data

Next we split the data into testing and training subsets

```{r}
inTrain <- createDataPartition(data$classe,
                               p=3/4,
                               list = FALSE);

training <- data[inTrain,];
testing <- data[-inTrain,];

summary(training);
```

It's apparent that there are quite a few missing values in some of the predictor variables so to help with model training we impute with a median.

```{r}
options(warn=-1)
preObj <- preProcess(training[,-156], method="medianImpute");
training.imputed <- predict(preObj, training);
```

## Training the model

Next we train our model using cross-validation, so the training set is split further into train/test subsets and the best model is selected, I've chosen to use a random forest for this classification problem, since it can handle co-linear, sparse, and non-normality in the predictor variables. Also, we can simplify the model later by analyzing the varibale importance (via varImp) post training and select a subset of significant predictors, which will also help us avoid overfitting.

```{r}
control <- trainControl(method='repeatedcv',
                        number=5,
                        repeats=3);


modFit <- train(classe ~ ., data=training.imputed, method="rf", trControl=control);
modFit
```

The accuracy of the model is extremely high at 0.9972 with a "mtry" value = 123 and all predictor variables in scope, however it's extremely unlikely that all predictor variables are significant and leaving them all in the model scope may lead to over-fitting, therefore we should take a look at predictor variable importance using varImp.

```{r}
#check the variable importance on the training set
rfImp <- varImp(modFit, scale=TRUE);
p = plot(rfImp, top=50);
p;
```

We keep only th top 20 predictors in the model and see what loss in accuracy results.

```{r}
#keep only the top 20 predictor variables in the model, this will help prevent over-fitting
p = plot(rfImp, top=20);
lst = c(p$y.limits, "classe");
training.optimized <- training.imputed[, (colnames(training.imputed) %in% lst)];

#re-train the model with only the top 20 predictors from our previous run
modFit <- train(classe ~ ., data=training.optimized, method="rf", trControl=control);
modFit
```

We can conclude that there is no loss in model accuracy after retaining only the top 20 predictor variables in the model. The article which this project is based on retains 17 predictors in the model but it's ok to be a little more greedy here.

## Testing the model
